{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Notebook\n",
    "\n",
    "In this notebook we have the entire pipeline of the project. Here, we use the models from [Hugging Face](https://huggingface.co/nlp-group-6) to be able to run our pipline. But if you would like to see how we trained the models, you can check the \"notebooks\" folder in the repository."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline\n",
    "\n",
    "Our pipeline consists of the following steps:\n",
    "1. Generate the question given the context and the answer.\n",
    "2. Generate the disractor given the question and the answer.\n",
    "\n",
    "This can be outline in image below:\n",
    "\n",
    "![pipeline](images/pipeline.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting asttokens==2.4.1\n",
      "  Using cached asttokens-2.4.1-py2.py3-none-any.whl (27 kB)\n",
      "Collecting certifi==2024.2.2\n",
      "  Using cached certifi-2024.2.2-py3-none-any.whl (163 kB)\n",
      "Collecting charset-normalizer==3.3.2\n",
      "  Using cached charset_normalizer-3.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (142 kB)\n",
      "Collecting comm==0.2.2\n",
      "  Using cached comm-0.2.2-py3-none-any.whl (7.2 kB)\n",
      "Collecting debugpy==1.8.1\n",
      "  Using cached debugpy-1.8.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "Collecting decorator==5.1.1\n",
      "  Using cached decorator-5.1.1-py3-none-any.whl (9.1 kB)\n",
      "Collecting exceptiongroup==1.2.0\n",
      "  Using cached exceptiongroup-1.2.0-py3-none-any.whl (16 kB)\n",
      "Collecting executing==2.0.1\n",
      "  Using cached executing-2.0.1-py2.py3-none-any.whl (24 kB)\n",
      "Collecting filelock==3.13.3\n",
      "  Using cached filelock-3.13.3-py3-none-any.whl (11 kB)\n",
      "Collecting fsspec==2024.3.1\n",
      "  Using cached fsspec-2024.3.1-py3-none-any.whl (171 kB)\n",
      "Collecting huggingface-hub==0.22.2\n",
      "  Using cached huggingface_hub-0.22.2-py3-none-any.whl (388 kB)\n",
      "Collecting idna==3.6\n",
      "  Using cached idna-3.6-py3-none-any.whl (61 kB)\n",
      "Collecting ipykernel==6.29.4\n",
      "  Using cached ipykernel-6.29.4-py3-none-any.whl (117 kB)\n",
      "Collecting ipython==8.23.0\n",
      "  Using cached ipython-8.23.0-py3-none-any.whl (814 kB)\n",
      "Collecting jedi==0.19.1\n",
      "  Using cached jedi-0.19.1-py2.py3-none-any.whl (1.6 MB)\n",
      "Collecting Jinja2==3.1.3\n",
      "  Using cached Jinja2-3.1.3-py3-none-any.whl (133 kB)\n",
      "Collecting jupyter_client==8.6.1\n",
      "  Using cached jupyter_client-8.6.1-py3-none-any.whl (105 kB)\n",
      "Collecting jupyter_core==5.7.2\n",
      "  Using cached jupyter_core-5.7.2-py3-none-any.whl (28 kB)\n",
      "Collecting MarkupSafe==2.1.5\n",
      "  Using cached MarkupSafe-2.1.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n",
      "Collecting matplotlib-inline==0.1.6\n",
      "  Using cached matplotlib_inline-0.1.6-py3-none-any.whl (9.4 kB)\n",
      "Collecting mpmath==1.3.0\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Collecting nest-asyncio==1.6.0\n",
      "  Using cached nest_asyncio-1.6.0-py3-none-any.whl (5.2 kB)\n",
      "Collecting networkx==3.3\n",
      "  Using cached networkx-3.3-py3-none-any.whl (1.7 MB)\n",
      "Collecting numpy==1.26.4\n",
      "  Using cached numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
      "Collecting nvidia-cublas-cu12==12.1.3.1\n",
      "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.1.105\n",
      "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.1.105\n",
      "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.1.105\n",
      "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
      "Collecting nvidia-cudnn-cu12==8.9.2.26\n",
      "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
      "Collecting nvidia-cufft-cu12==11.0.2.54\n",
      "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
      "Collecting nvidia-curand-cu12==10.3.2.106\n",
      "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
      "Collecting nvidia-cusolver-cu12==11.4.5.107\n",
      "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
      "Collecting nvidia-cusparse-cu12==12.1.0.106\n",
      "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
      "Collecting nvidia-nccl-cu12==2.19.3\n",
      "  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127\n",
      "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "Collecting nvidia-nvtx-cu12==12.1.105\n",
      "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
      "Collecting packaging==24.0\n",
      "  Using cached packaging-24.0-py3-none-any.whl (53 kB)\n",
      "Collecting parso==0.8.4\n",
      "  Using cached parso-0.8.4-py2.py3-none-any.whl (103 kB)\n",
      "Collecting pexpect==4.9.0\n",
      "  Using cached pexpect-4.9.0-py2.py3-none-any.whl (63 kB)\n",
      "Collecting pillow==10.3.0\n",
      "  Using cached pillow-10.3.0-cp310-cp310-manylinux_2_28_x86_64.whl (4.5 MB)\n",
      "Collecting platformdirs==4.2.0\n",
      "  Using cached platformdirs-4.2.0-py3-none-any.whl (17 kB)\n",
      "Collecting prompt-toolkit==3.0.43\n",
      "  Using cached prompt_toolkit-3.0.43-py3-none-any.whl (386 kB)\n",
      "Collecting psutil==5.9.8\n",
      "  Using cached psutil-5.9.8-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (288 kB)\n",
      "Collecting ptyprocess==0.7.0\n",
      "  Using cached ptyprocess-0.7.0-py2.py3-none-any.whl (13 kB)\n",
      "Collecting pure-eval==0.2.2\n",
      "  Using cached pure_eval-0.2.2-py3-none-any.whl (11 kB)\n",
      "Collecting Pygments==2.17.2\n",
      "  Using cached pygments-2.17.2-py3-none-any.whl (1.2 MB)\n",
      "Collecting python-dateutil==2.9.0.post0\n",
      "  Using cached python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n",
      "Collecting PyYAML==6.0.1\n",
      "  Using cached PyYAML-6.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (705 kB)\n",
      "Collecting pyzmq==25.1.2\n",
      "  Using cached pyzmq-25.1.2-cp310-cp310-manylinux_2_28_x86_64.whl (1.1 MB)\n",
      "Collecting regex==2023.12.25\n",
      "  Using cached regex-2023.12.25-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (773 kB)\n",
      "Collecting requests==2.31.0\n",
      "  Using cached requests-2.31.0-py3-none-any.whl (62 kB)\n",
      "Collecting safetensors==0.4.2\n",
      "  Using cached safetensors-0.4.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "Collecting six==1.16.0\n",
      "  Using cached six-1.16.0-py2.py3-none-any.whl (11 kB)\n",
      "Collecting stack-data==0.6.3\n",
      "  Using cached stack_data-0.6.3-py3-none-any.whl (24 kB)\n",
      "Collecting sympy==1.12\n",
      "  Using cached sympy-1.12-py3-none-any.whl (5.7 MB)\n",
      "Collecting tokenizers==0.15.2\n",
      "  Using cached tokenizers-0.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "Collecting torch==2.2.2\n",
      "  Using cached torch-2.2.2-cp310-cp310-manylinux1_x86_64.whl (755.5 MB)\n",
      "Collecting torchaudio==2.2.2\n",
      "  Using cached torchaudio-2.2.2-cp310-cp310-manylinux1_x86_64.whl (3.3 MB)\n",
      "Collecting torchvision==0.17.2\n",
      "  Using cached torchvision-0.17.2-cp310-cp310-manylinux1_x86_64.whl (6.9 MB)\n",
      "Collecting tornado==6.4\n",
      "  Using cached tornado-6.4-cp38-abi3-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (435 kB)\n",
      "Collecting tqdm==4.66.2\n",
      "  Using cached tqdm-4.66.2-py3-none-any.whl (78 kB)\n",
      "Collecting traitlets==5.14.2\n",
      "  Using cached traitlets-5.14.2-py3-none-any.whl (85 kB)\n",
      "Collecting transformers==4.39.3\n",
      "  Using cached transformers-4.39.3-py3-none-any.whl (8.8 MB)\n",
      "Collecting triton==2.2.0\n",
      "  Using cached triton-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (167.9 MB)\n",
      "Collecting typing_extensions==4.11.0\n",
      "  Using cached typing_extensions-4.11.0-py3-none-any.whl (34 kB)\n",
      "Collecting urllib3==2.2.1\n",
      "  Using cached urllib3-2.2.1-py3-none-any.whl (121 kB)\n",
      "Collecting wcwidth==0.2.13\n",
      "  Using cached wcwidth-0.2.13-py2.py3-none-any.whl (34 kB)\n",
      "Installing collected packages: wcwidth, pure-eval, ptyprocess, mpmath, urllib3, typing_extensions, traitlets, tqdm, tornado, sympy, six, safetensors, regex, pyzmq, PyYAML, Pygments, psutil, prompt-toolkit, platformdirs, pillow, pexpect, parso, packaging, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, networkx, nest-asyncio, MarkupSafe, idna, fsspec, filelock, executing, exceptiongroup, decorator, debugpy, charset-normalizer, certifi, triton, requests, python-dateutil, nvidia-cusparse-cu12, nvidia-cudnn-cu12, matplotlib-inline, jupyter_core, Jinja2, jedi, comm, asttokens, stack-data, nvidia-cusolver-cu12, jupyter_client, huggingface-hub, torch, tokenizers, ipython, transformers, torchvision, torchaudio, ipykernel\n",
      "Successfully installed Jinja2-3.1.3 MarkupSafe-2.1.5 PyYAML-6.0.1 Pygments-2.17.2 asttokens-2.4.1 certifi-2024.2.2 charset-normalizer-3.3.2 comm-0.2.2 debugpy-1.8.1 decorator-5.1.1 exceptiongroup-1.2.0 executing-2.0.1 filelock-3.13.3 fsspec-2024.3.1 huggingface-hub-0.22.2 idna-3.6 ipykernel-6.29.4 ipython-8.23.0 jedi-0.19.1 jupyter_client-8.6.1 jupyter_core-5.7.2 matplotlib-inline-0.1.6 mpmath-1.3.0 nest-asyncio-1.6.0 networkx-3.3 numpy-1.26.4 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105 packaging-24.0 parso-0.8.4 pexpect-4.9.0 pillow-10.3.0 platformdirs-4.2.0 prompt-toolkit-3.0.43 psutil-5.9.8 ptyprocess-0.7.0 pure-eval-0.2.2 python-dateutil-2.9.0.post0 pyzmq-25.1.2 regex-2023.12.25 requests-2.31.0 safetensors-0.4.2 six-1.16.0 stack-data-0.6.3 sympy-1.12 tokenizers-0.15.2 torch-2.2.2 torchaudio-2.2.2 torchvision-0.17.2 tornado-6.4 tqdm-4.66.2 traitlets-5.14.2 transformers-4.39.3 triton-2.2.0 typing_extensions-4.11.0 urllib3-2.2.1 wcwidth-0.2.13\n"
     ]
    }
   ],
   "source": [
    "! pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we import the transformers library from Hugging Face and the models we will use in the pipeline. We use both the BartTokenizer and BartForConditionalGeneration models from the transformers library, since we are using the Bart model for both the question generation and the distractor generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BartTokenizer, BartForConditionalGeneration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we start with the question generator model. We can easily load the model from the Hugging Face library by using the `BartForConditionalGeneration.from_pretrained` method. We also load the tokenizer for the model. Ofcourse given the link to the model in the Hugging Face library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_model = BartForConditionalGeneration.from_pretrained('nlp-group-6/sciq-question-generator')\n",
    "question_tokenizer = BartTokenizer.from_pretrained('nlp-group-6/sciq-question-generator')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we load the disractor generator model. We use the same methods as before to load the model and the tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "options_model = BartForConditionalGeneration.from_pretrained('nlp-group-6/sciq-options-generator-generated-questions')\n",
    "options_tokenizer = BartTokenizer.from_pretrained('nlp-group-6/sciq-options-generator-generated-questions')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example\n",
    "\n",
    "Now we put the pipeline together and test it with a simple example. We know that give the support and the correct answer we need to generate the question and the distractors. So lets define the support and the correct answer first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "support = \"Milk has a white color.\"\n",
    "correct_answer = \"Milk is white\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to generate the question, we need to tokenize the support and the correct answer. So we use the tokenizer of the model and then put those as inputs to the model to generate the question. We can then decode the output to get the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mohammad/Uni/2023-2024/Student/NLP/project/.venv/lib/python3.10/site-packages/transformers/generation/utils.py:1132: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "inputs = question_tokenizer(support, correct_answer, return_tensors='pt', max_length=512, truncation=True)\n",
    "question = question_model.generate(**inputs)\n",
    "question = question_tokenizer.decode(question[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets print the question to see what our brilliant model has come up with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What color is milk?\n"
     ]
    }
   ],
   "source": [
    "print(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfect! We now need to plug the question, correct answer and support into the options generator. But before we need to bundle them together with the BART sentence stopper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_answer_context = [question + \"</s><s>\" + correct_answer + \"</s><s>\" + support]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the options\n",
    "inputs = options_tokenizer(question_answer_context, return_tensors='pt', max_length=512, truncation=True)\n",
    "options = options_model.generate(**inputs, num_return_sequences=4, num_beams=4, max_length=50)\n",
    "options = options_tokenizer.batch_decode(options, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets see what the model has come up with for the distractors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Milk is red', 'Milk is orange', 'milk is red', 'Milk is black']\n"
     ]
    }
   ],
   "source": [
    "print(options)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets put everything together and see the final output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question : What color is milk?\n",
      "a) Milk is red\n",
      "b) Milk is orange\n",
      "c) milk is red\n",
      "d) Milk is black\n",
      "e) Milk is white\n"
     ]
    }
   ],
   "source": [
    "print(\"Question : \" + question)\n",
    "indices = ['a', 'b', 'c', 'd', 'e']\n",
    "\n",
    "for option in options:\n",
    "    if option == correct_answer:\n",
    "        continue\n",
    "    print(indices.pop(0) + \") \" + option)\n",
    "    \n",
    "print(indices.pop(0) + \") \" + correct_answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
