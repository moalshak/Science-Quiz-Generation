{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd34315",
   "metadata": {},
   "source": [
    "First we import the necessary libraries. We need torch, datasets and transformers. We will use the BartTokenizer and BartForConditionalGeneration from the transformers library. And of course the Seq2SeqTrainer and TrainingArguments from the transformers library to train our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a864afdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mohammad/Uni/2023-2024/Student/NLP/project/notebooks/generation/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "from transformers import BartForConditionalGeneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "806a8ffa90f45fd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# make sure to include cuda\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('facebook/bart-base')\n",
    "\n",
    "data = load_dataset(\"allenai/sciq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba257761377cc459",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_input = 512\n",
    "max_target = 128\n",
    "batch_size = 36"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6882ca6a2ca751",
   "metadata": {},
   "outputs": [],
   "source": [
    "args1 = Seq2SeqTrainingArguments(\n",
    "        output_dir=\"./results_prediction_question\",\n",
    "        evaluation_strategy='epoch',\n",
    "        learning_rate=2e-5,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        gradient_accumulation_steps=2,\n",
    "        weight_decay=0.01,\n",
    "        save_total_limit=2,\n",
    "        num_train_epochs=32,\n",
    "        predict_with_generate=True,\n",
    "        eval_accumulation_steps=32,\n",
    "        fp16=torch.cuda.is_available()  # available only with CUDA\n",
    "    )\n",
    "\n",
    "MODEL_FOLDER = \"models/sciq\"\n",
    "\n",
    "questions_model = BartForConditionalGeneration.from_pretrained(f\"./{MODEL_FOLDER}\")\n",
    "questions_trainer = Seq2SeqTrainer(\n",
    "    questions_model,\n",
    "    args1,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390bd41106ed40b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process_data_question_model(data):\n",
    "    # tokenize the data\n",
    "    inputs = tokenizer(data['support'], data['correct_answer'], padding=\"max_length\", truncation=True, max_length=max_input, return_tensors=\"pt\")\n",
    "    targets = tokenizer(data['question'], padding=\"max_length\", truncation=True, max_length=max_target, return_tensors=\"pt\")\n",
    "    return {\"input_ids\": inputs.input_ids, \"attention_mask\": inputs.attention_mask, \"labels\": targets.input_ids}\n",
    "\n",
    "def add_generated_question(dataset):\n",
    "    question_data = dataset.map(pre_process_data_question_model, batched=True)\n",
    "    predictions = questions_trainer.predict(question_data, max_length=64)\n",
    "    valid_tokens = []\n",
    "    for prediction in predictions[0]:\n",
    "        valid_tokens.append([token for token in prediction if token != -100])\n",
    "    generated_questions = tokenizer.batch_decode(valid_tokens, skip_special_tokens=True)\n",
    "    return dataset.add_column(\"generated_question\", generated_questions)\n",
    "\n",
    "data['validation'] = add_generated_question(data['validation'])\n",
    "data['train'] = add_generated_question(data['train'])\n",
    "data['test'] = add_generated_question(data['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac9fe838d7d98f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.save_to_disk(\"datasets/generated-questions.hf\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
