{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Options Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we import the necessary libraries. We need torch, datasets and transformers. We will use the BartTokenizer and BartForConditionalGeneration from the transformers library. And of course the Seq2SeqTrainer and TrainingArguments from the transformers library to train our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "from transformers import BartForConditionalGeneration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure to include cuda, otherwise we are training on the cpu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we load our base model and its tokenizer using the BartTokenizer and BartForConditionalGeneration classes. The model is loaded from the 'facebook/bart-base' checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-06T17:02:39.245757Z",
     "start_time": "2024-04-06T17:02:35.525365Z"
    }
   },
   "outputs": [],
   "source": [
    "model = BartForConditionalGeneration.from_pretrained('facebook/bart-base')\n",
    "tokenizer = AutoTokenizer.from_pretrained('facebook/bart-base')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the questions dataset from where we saved it on our Hugging Face account. See the generated-questions-dataset notebook for more information on how to create this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-06T17:02:44.681235Z",
     "start_time": "2024-04-06T17:02:39.246797Z"
    }
   },
   "outputs": [],
   "source": [
    "data = load_dataset(\"nlp-group-6/sciq-with-generated-questions\")\n",
    "train_data = data['train']\n",
    "val_data = data['validation']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define some args which are the limitations to the model and our hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-06T17:02:47.828348Z",
     "start_time": "2024-04-06T17:02:47.824936Z"
    }
   },
   "outputs": [],
   "source": [
    "max_input = 512\n",
    "max_target = 128\n",
    "batch_size = 36"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will define a `pre_process_data` function to preprocess the data. This function will tokenize the data and return the tokenized data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-06T17:02:53.919747Z",
     "start_time": "2024-04-06T17:02:49.210281Z"
    }
   },
   "outputs": [],
   "source": [
    "def pre_process_data(data):\n",
    "    # combine the question, correct answer and support text\n",
    "    # we use the <s> token to separate the question, correct answer and support text\n",
    "    question_answer_context = [question + \"</s><s>\" + correct_answer + \"</s><s>\" + support for question, correct_answer, support in zip(data['generated_question'], data['correct_answer'], data['support'])]\n",
    "    \n",
    "    # tokenize the data\n",
    "    inputs = tokenizer(question_answer_context, padding=\"max_length\", truncation=True, max_length=max_input, return_tensors=\"pt\")\n",
    "    targets = tokenizer(data['distractor1'], data['distractor2'], data['distractor3'], padding=\"max_length\", truncation=True, max_length=max_target, return_tensors=\"pt\")\n",
    "    return {\"input_ids\": inputs.input_ids, \"attention_mask\": inputs.attention_mask, \"labels\": targets.input_ids}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we map the entire dataset to our `pre_process_data` function, batch and shuffle the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.map(pre_process_data, batched=True).shuffle(seed=42)\n",
    "val_data = val_data.map(pre_process_data, batched=True).shuffle(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-06T17:02:55.301034Z",
     "start_time": "2024-04-06T17:02:55.297995Z"
    }
   },
   "outputs": [],
   "source": [
    "# empty memory, just in case\n",
    "torch.cuda.empty_cache()\n",
    "# put the model on the device, hopefully the GPU\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define our seq2seq trainer and training arguments, more details about this in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-06T17:02:56.151205Z",
     "start_time": "2024-04-06T17:02:55.978204Z"
    }
   },
   "outputs": [],
   "source": [
    "args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./results_option_generation\",\n",
    "    evaluation_strategy='epoch',\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size= batch_size,\n",
    "    gradient_accumulation_steps=2,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=2,\n",
    "    num_train_epochs=32,\n",
    "    predict_with_generate=True,\n",
    "    eval_accumulation_steps=32,\n",
    "    fp16=True #available only with CUDA\n",
    ")\n",
    "\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model, \n",
    "    args,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=val_data,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we save the model and tokenizer to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUT_DIR = \"sciq_options1_generator\"\n",
    "model.save_pretrained(OUT_DIR)\n",
    "tokenizer.save_pretrained(OUT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, to test things out we load it and generate some options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "\n",
    "model = BartForConditionalGeneration.from_pretrained(f\"./{OUT_DIR}\")\n",
    "tokenizer = BartTokenizer.from_pretrained(f\"./{OUT_DIR}\")\n",
    "# put them both on the same device\n",
    "_ = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['particle accelerators', 'kinetic accelerators', 'neutron accelerators']\n"
     ]
    }
   ],
   "source": [
    "input_text = \"What amazing machines smash particles that are smaller than atoms into each other head-on?\"\n",
    "correct_answer = \"particle accelerators\"\n",
    "\n",
    "input_ids = tokenizer(input_text, correct_answer, return_tensors=\"pt\").input_ids.to(device)\n",
    "output = model.generate(input_ids, max_length=128, num_beams=4, num_return_sequences=3, early_stopping=True)\n",
    "outputs = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False) for g in output]\n",
    "print(outputs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
