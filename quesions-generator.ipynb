{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cu121\n",
      "Requirement already satisfied: torch in c:\\users\\p306726\\appdata\\roaming\\python\\python311\\site-packages (2.2.1+cu121)\n",
      "Requirement already satisfied: torchvision in c:\\users\\p306726\\appdata\\roaming\\python\\python311\\site-packages (0.17.1+cu121)\n",
      "Requirement already satisfied: torchaudio in c:\\users\\p306726\\appdata\\roaming\\python\\python311\\site-packages (2.2.1+cu121)\n",
      "Requirement already satisfied: filelock in c:\\python311\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\tools\\manim\\lib\\site-packages (from torch) (4.9.0)\n",
      "Requirement already satisfied: sympy in c:\\python311\\lib\\site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\tools\\manim\\lib\\site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\p306726\\appdata\\roaming\\python\\python311\\site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in c:\\python311\\lib\\site-packages (from torch) (2024.2.0)\n",
      "Requirement already satisfied: numpy in c:\\tools\\manim\\lib\\site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\tools\\manim\\lib\\site-packages (from torchvision) (9.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\python311\\lib\\site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\python311\\lib\\site-packages (from sympy->torch) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-19T14:14:03.235920Z",
     "start_time": "2024-03-19T14:14:00.727423Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "# import BartForConditionalGeneration\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "\n",
    "# make sure to include cuda\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "model = BartForConditionalGeneration.from_pretrained('facebook/bart-base')\n",
    "tokenizer = AutoTokenizer.from_pretrained('facebook/bart-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-19T14:16:16.095136Z",
     "start_time": "2024-03-19T14:16:09.419960Z"
    }
   },
   "outputs": [],
   "source": [
    "data = load_dataset(\"allenai/sciq\")\n",
    "train_data = data['train']\n",
    "eval_data = data['test']\n",
    "test_data = data['validation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-19T14:13:46.880023Z",
     "start_time": "2024-03-19T14:13:46.877338Z"
    }
   },
   "outputs": [],
   "source": [
    "max_input = 512\n",
    "max_target = 128\n",
    "batch_size = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-19T14:25:13.562522Z",
     "start_time": "2024-03-19T14:25:10.898999Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/11679 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "282b5b6bf6904f7391507e7b1b3f3e60"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c4d686a56c8f4b4085d4d8d64ba7b7e2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "99780ba0c4074380a7042951874fec86"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# dataset has:\n",
    "# question, distractor3, distractor1, distractor2, correct_answer, support\n",
    "def pre_process_data(data):\n",
    "    # tokenize the data\n",
    "    inputs = tokenizer(data['support'], padding=\"max_length\", truncation=True, max_length=max_input, return_tensors=\"pt\")\n",
    "    targets = tokenizer(data['question'], padding=\"max_length\", truncation=True, max_length=max_target, return_tensors=\"pt\")\n",
    "    return {\"input_ids\": inputs.input_ids, \"attention_mask\": inputs.attention_mask, \"labels\": targets.input_ids}\n",
    "\n",
    "train_data = train_data.map(pre_process_data, batched=True).shuffle(seed=42).select(range(1000))\n",
    "eval_data = eval_data.map(pre_process_data, batched=True).shuffle(seed=42).select(range(100))\n",
    "test_data = test_data.map(pre_process_data, batched=True).shuffle(seed=42).select(range(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# empty memory\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\P306726\\Uni\\2023-2024\\Student\\NLP\\project\\.venv\\Lib\\site-packages\\accelerate\\accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n",
      "  0%|          | 0/8000 [00:00<?, ?it/s]c:\\Users\\P306726\\Uni\\2023-2024\\Student\\NLP\\project\\.venv\\Lib\\site-packages\\transformers\\models\\bart\\modeling_bart.py:590: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:263.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
      "                                                  \n",
      "  3%|▎         | 250/8000 [01:21<42:56,  3.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.23404675722122192, 'eval_runtime': 2.0107, 'eval_samples_per_second': 49.734, 'eval_steps_per_second': 24.867, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▋         | 500/8000 [02:43<40:26,  3.09it/s]  Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4821, 'grad_norm': 1.7026487588882446, 'learning_rate': 1.876e-05, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      "  6%|▋         | 500/8000 [02:49<40:26,  3.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.21688079833984375, 'eval_runtime': 1.9602, 'eval_samples_per_second': 51.016, 'eval_steps_per_second': 25.508, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \n",
      "  9%|▉         | 750/8000 [04:10<36:20,  3.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.21881717443466187, 'eval_runtime': 1.7749, 'eval_samples_per_second': 56.34, 'eval_steps_per_second': 28.17, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▎        | 1000/8000 [05:28<37:08,  3.14it/s] Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1481, 'grad_norm': 1.3198992013931274, 'learning_rate': 1.751e-05, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 12%|█▎        | 1000/8000 [05:32<37:08,  3.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.22326040267944336, 'eval_runtime': 1.7272, 'eval_samples_per_second': 57.898, 'eval_steps_per_second': 28.949, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 16%|█▌        | 1250/8000 [06:52<33:52,  3.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.23713943362236023, 'eval_runtime': 1.7609, 'eval_samples_per_second': 56.79, 'eval_steps_per_second': 28.395, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 1500/8000 [08:09<30:17,  3.58it/s]  Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1033, 'grad_norm': 0.8450888991355896, 'learning_rate': 1.626e-05, 'epoch': 6.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 19%|█▉        | 1500/8000 [08:13<30:17,  3.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.24260342121124268, 'eval_runtime': 1.6307, 'eval_samples_per_second': 61.322, 'eval_steps_per_second': 30.661, 'epoch': 6.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 22%|██▏       | 1750/8000 [09:27<29:08,  3.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2525208294391632, 'eval_runtime': 1.6318, 'eval_samples_per_second': 61.283, 'eval_steps_per_second': 30.641, 'epoch': 7.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 2000/8000 [10:39<27:47,  3.60it/s]  Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0738, 'grad_norm': 1.6719452142715454, 'learning_rate': 1.501e-05, 'epoch': 8.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 25%|██▌       | 2000/8000 [10:42<27:47,  3.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.26093629002571106, 'eval_runtime': 1.6162, 'eval_samples_per_second': 61.874, 'eval_steps_per_second': 30.937, 'epoch': 8.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 28%|██▊       | 2250/8000 [11:58<29:43,  3.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.27157336473464966, 'eval_runtime': 1.7333, 'eval_samples_per_second': 57.695, 'eval_steps_per_second': 28.847, 'epoch': 9.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███▏      | 2500/8000 [13:17<30:34,  3.00it/s]  Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0522, 'grad_norm': 0.9570529460906982, 'learning_rate': 1.37625e-05, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 31%|███▏      | 2500/8000 [13:22<30:34,  3.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2749168872833252, 'eval_runtime': 2.1786, 'eval_samples_per_second': 45.9, 'eval_steps_per_second': 22.95, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 34%|███▍      | 2750/8000 [14:44<27:15,  3.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2830430269241333, 'eval_runtime': 1.7495, 'eval_samples_per_second': 57.158, 'eval_steps_per_second': 28.579, 'epoch': 11.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 3000/8000 [16:04<27:24,  3.04it/s]  Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0384, 'grad_norm': 1.583770513534546, 'learning_rate': 1.2512500000000001e-05, 'epoch': 12.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 38%|███▊      | 3000/8000 [16:08<27:24,  3.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2863128185272217, 'eval_runtime': 1.7679, 'eval_samples_per_second': 56.563, 'eval_steps_per_second': 28.282, 'epoch': 12.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 41%|████      | 3250/8000 [17:29<25:53,  3.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.30117201805114746, 'eval_runtime': 1.9748, 'eval_samples_per_second': 50.638, 'eval_steps_per_second': 25.319, 'epoch': 13.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 3500/8000 [18:49<23:06,  3.25it/s]  Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0285, 'grad_norm': 1.838121771812439, 'learning_rate': 1.1262500000000001e-05, 'epoch': 14.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 44%|████▍     | 3500/8000 [18:54<23:06,  3.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2988032102584839, 'eval_runtime': 1.9138, 'eval_samples_per_second': 52.252, 'eval_steps_per_second': 26.126, 'epoch': 14.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 47%|████▋     | 3750/8000 [20:12<19:45,  3.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3071799874305725, 'eval_runtime': 1.6178, 'eval_samples_per_second': 61.811, 'eval_steps_per_second': 30.905, 'epoch': 15.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 4000/8000 [21:24<18:50,  3.54it/s]Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0222, 'grad_norm': 1.8917053937911987, 'learning_rate': 1.0012500000000001e-05, 'epoch': 16.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 50%|█████     | 4000/8000 [21:28<18:50,  3.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.31092241406440735, 'eval_runtime': 1.6871, 'eval_samples_per_second': 59.274, 'eval_steps_per_second': 29.637, 'epoch': 16.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 53%|█████▎    | 4250/8000 [22:40<17:02,  3.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.31307610869407654, 'eval_runtime': 1.6017, 'eval_samples_per_second': 62.432, 'eval_steps_per_second': 31.216, 'epoch': 17.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▋    | 4500/8000 [23:51<16:38,  3.51it/s]Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0177, 'grad_norm': 0.8407136797904968, 'learning_rate': 8.762500000000001e-06, 'epoch': 18.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 56%|█████▋    | 4500/8000 [23:55<16:38,  3.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3142528831958771, 'eval_runtime': 1.5696, 'eval_samples_per_second': 63.712, 'eval_steps_per_second': 31.856, 'epoch': 18.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 59%|█████▉    | 4750/8000 [25:07<15:42,  3.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.31991955637931824, 'eval_runtime': 1.6328, 'eval_samples_per_second': 61.245, 'eval_steps_per_second': 30.622, 'epoch': 19.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▎   | 5000/8000 [26:18<15:31,  3.22it/s]Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.015, 'grad_norm': 0.9104005694389343, 'learning_rate': 7.5125000000000005e-06, 'epoch': 20.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 62%|██████▎   | 5000/8000 [26:22<15:31,  3.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.31863442063331604, 'eval_runtime': 1.9455, 'eval_samples_per_second': 51.402, 'eval_steps_per_second': 25.701, 'epoch': 20.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 66%|██████▌   | 5250/8000 [27:36<12:49,  3.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3220687508583069, 'eval_runtime': 1.6168, 'eval_samples_per_second': 61.85, 'eval_steps_per_second': 30.925, 'epoch': 21.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 5500/8000 [28:47<11:42,  3.56it/s]Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0135, 'grad_norm': 1.3722989559173584, 'learning_rate': 6.262500000000001e-06, 'epoch': 22.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 69%|██████▉   | 5500/8000 [28:50<11:42,  3.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3214161992073059, 'eval_runtime': 1.6299, 'eval_samples_per_second': 61.354, 'eval_steps_per_second': 30.677, 'epoch': 22.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 72%|███████▏  | 5750/8000 [30:03<10:21,  3.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.32765263319015503, 'eval_runtime': 1.6153, 'eval_samples_per_second': 61.907, 'eval_steps_per_second': 30.954, 'epoch': 23.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 6000/8000 [31:15<09:15,  3.60it/s]Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0121, 'grad_norm': 0.8747310638427734, 'learning_rate': 5.0125e-06, 'epoch': 24.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 75%|███████▌  | 6000/8000 [31:19<09:15,  3.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.32277077436447144, 'eval_runtime': 1.696, 'eval_samples_per_second': 58.963, 'eval_steps_per_second': 29.481, 'epoch': 24.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 78%|███████▊  | 6250/8000 [32:31<08:22,  3.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3261013925075531, 'eval_runtime': 1.6974, 'eval_samples_per_second': 58.915, 'eval_steps_per_second': 29.457, 'epoch': 25.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████▏ | 6500/8000 [33:46<08:23,  2.98it/s]Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0107, 'grad_norm': 0.3925653398036957, 'learning_rate': 3.7625e-06, 'epoch': 26.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 81%|████████▏ | 6500/8000 [33:50<08:23,  2.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.33099183440208435, 'eval_runtime': 1.8394, 'eval_samples_per_second': 54.365, 'eval_steps_per_second': 27.182, 'epoch': 26.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 84%|████████▍ | 6750/8000 [35:15<06:57,  2.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3315889835357666, 'eval_runtime': 1.9, 'eval_samples_per_second': 52.632, 'eval_steps_per_second': 26.316, 'epoch': 27.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 7000/8000 [36:38<05:32,  3.01it/s]Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0101, 'grad_norm': 0.5510970950126648, 'learning_rate': 2.5125e-06, 'epoch': 28.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 88%|████████▊ | 7000/8000 [36:43<05:32,  3.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.33281752467155457, 'eval_runtime': 1.8233, 'eval_samples_per_second': 54.847, 'eval_steps_per_second': 27.423, 'epoch': 28.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 91%|█████████ | 7250/8000 [38:07<04:04,  3.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3364467918872833, 'eval_runtime': 1.8872, 'eval_samples_per_second': 52.987, 'eval_steps_per_second': 26.494, 'epoch': 29.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 7500/8000 [39:30<02:46,  3.00it/s]Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0091, 'grad_norm': 0.24454258382320404, 'learning_rate': 1.2625000000000002e-06, 'epoch': 30.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 94%|█████████▍| 7500/8000 [39:35<02:46,  3.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.33470889925956726, 'eval_runtime': 1.8545, 'eval_samples_per_second': 53.924, 'eval_steps_per_second': 26.962, 'epoch': 30.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 97%|█████████▋| 7750/8000 [40:59<01:23,  3.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3345467746257782, 'eval_runtime': 1.8695, 'eval_samples_per_second': 53.49, 'eval_steps_per_second': 26.745, 'epoch': 31.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8000/8000 [42:24<00:00,  2.85it/s]Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0095, 'grad_norm': 0.2064138799905777, 'learning_rate': 1.5000000000000002e-08, 'epoch': 32.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      "100%|██████████| 8000/8000 [42:28<00:00,  3.14it/s]\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3348207175731659, 'eval_runtime': 1.9055, 'eval_samples_per_second': 52.48, 'eval_steps_per_second': 26.24, 'epoch': 32.0}\n",
      "{'train_runtime': 2548.7872, 'train_samples_per_second': 12.555, 'train_steps_per_second': 3.139, 'train_loss': 0.12788135969638825, 'epoch': 32.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('sciq_model\\\\tokenizer_config.json',\n",
       " 'sciq_model\\\\special_tokens_map.json',\n",
       " 'sciq_model\\\\vocab.json',\n",
       " 'sciq_model\\\\merges.txt',\n",
       " 'sciq_model\\\\added_tokens.json',\n",
       " 'sciq_model\\\\tokenizer.json')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: add versioning\n",
    "\n",
    "model.to(device)\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy='epoch',\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size= batch_size,\n",
    "    gradient_accumulation_steps=2,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=2,\n",
    "    num_train_epochs=32,\n",
    "    predict_with_generate=True,\n",
    "    eval_accumulation_steps=32,\n",
    "    fp16=True #available only with CUDA\n",
    ")\n",
    "\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model, \n",
    "    args,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=eval_data,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "# lets save the model\n",
    "OUT_DIR = \"sciq\"\n",
    "model.save_pretrained(OUT_DIR)\n",
    "tokenizer.save_pretrained(OUT_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_FOLDER = \"sciq\"\n",
    "model = BartForConditionalGeneration.from_pretrained(f\"./{MODEL_FOLDER}\")\n",
    "tokenizer = BartTokenizer.from_pretrained(f\"./{MODEL_FOLDER}\")\n",
    "# put them both on the same device\n",
    "_ = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What color is milk?\n"
     ]
    }
   ],
   "source": [
    "# now lets test it with an input\n",
    "input_text = \"Milk has a white color.\"\n",
    "inputs = tokenizer(input_text, padding=\"max_length\", truncation=True, max_length=max_input, return_tensors=\"pt\")\n",
    "inputs = {k: inputs[k].to(device) for k in inputs}\n",
    "\n",
    "result = model.generate(**inputs)\n",
    "output = tokenizer.decode(result[0], skip_special_tokens=True)\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/roben/Documents/courses/NLP/Science-Quiz-Generation/venv/lib/python3.10/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = BartForConditionalGeneration.from_pretrained('nlp-group-6/sciq-question-generator', token=\"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\")\n",
    "\n",
    "model.to(device)\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy='epoch',\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size= batch_size,\n",
    "    gradient_accumulation_steps=2,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=2,\n",
    "    num_train_epochs=32,\n",
    "    predict_with_generate=True,\n",
    "    eval_accumulation_steps=32\n",
    "    # fp16=True #available only with CUDA\n",
    ")\n",
    "\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model, \n",
    "    args,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=eval_data,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-19T14:14:25.833564Z",
     "start_time": "2024-03-19T14:14:24.410640Z"
    }
   },
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n    <div>\n      \n      <progress value='21' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [21/50 21:25 < 31:04, 0.02 it/s]\n    </div>\n    "
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[68], line 13\u001B[0m\n\u001B[1;32m     10\u001B[0m     total_bleu_score \u001B[38;5;241m/\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlen\u001B[39m(targets)\n\u001B[1;32m     11\u001B[0m     \u001B[38;5;28mprint\u001B[39m(total_bleu_score)\n\u001B[0;32m---> 13\u001B[0m predictions \u001B[38;5;241m=\u001B[39m \u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpredict\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtest_data\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     14\u001B[0m compute_bleu_score(test_data, predictions[\u001B[38;5;241m0\u001B[39m])\n",
      "File \u001B[0;32m~/Documents/courses/NLP/Science-Quiz-Generation/venv/lib/python3.10/site-packages/transformers/trainer_seq2seq.py:230\u001B[0m, in \u001B[0;36mSeq2SeqTrainer.predict\u001B[0;34m(self, test_dataset, ignore_keys, metric_key_prefix, **gen_kwargs)\u001B[0m\n\u001B[1;32m    227\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgather_function \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maccelerator\u001B[38;5;241m.\u001B[39mgather\n\u001B[1;32m    228\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_gen_kwargs \u001B[38;5;241m=\u001B[39m gen_kwargs\n\u001B[0;32m--> 230\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpredict\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtest_dataset\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mignore_keys\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mignore_keys\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmetric_key_prefix\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmetric_key_prefix\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/courses/NLP/Science-Quiz-Generation/venv/lib/python3.10/site-packages/transformers/trainer.py:3305\u001B[0m, in \u001B[0;36mTrainer.predict\u001B[0;34m(self, test_dataset, ignore_keys, metric_key_prefix)\u001B[0m\n\u001B[1;32m   3302\u001B[0m start_time \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime()\n\u001B[1;32m   3304\u001B[0m eval_loop \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprediction_loop \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39muse_legacy_prediction_loop \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mevaluation_loop\n\u001B[0;32m-> 3305\u001B[0m output \u001B[38;5;241m=\u001B[39m \u001B[43meval_loop\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   3306\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtest_dataloader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdescription\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mPrediction\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mignore_keys\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mignore_keys\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmetric_key_prefix\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmetric_key_prefix\u001B[49m\n\u001B[1;32m   3307\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   3308\u001B[0m total_batch_size \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39meval_batch_size \u001B[38;5;241m*\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39mworld_size\n\u001B[1;32m   3309\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmetric_key_prefix\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m_jit_compilation_time\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m output\u001B[38;5;241m.\u001B[39mmetrics:\n",
      "File \u001B[0;32m~/Documents/courses/NLP/Science-Quiz-Generation/venv/lib/python3.10/site-packages/transformers/trainer.py:3418\u001B[0m, in \u001B[0;36mTrainer.evaluation_loop\u001B[0;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001B[0m\n\u001B[1;32m   3415\u001B[0m         batch_size \u001B[38;5;241m=\u001B[39m observed_batch_size\n\u001B[1;32m   3417\u001B[0m \u001B[38;5;66;03m# Prediction step\u001B[39;00m\n\u001B[0;32m-> 3418\u001B[0m loss, logits, labels \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mprediction_step\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mprediction_loss_only\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mignore_keys\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mignore_keys\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   3419\u001B[0m main_input_name \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mgetattr\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmain_input_name\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minput_ids\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m   3420\u001B[0m inputs_decode \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_prepare_input(inputs[main_input_name]) \u001B[38;5;28;01mif\u001B[39;00m args\u001B[38;5;241m.\u001B[39minclude_inputs_for_metrics \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/Documents/courses/NLP/Science-Quiz-Generation/venv/lib/python3.10/site-packages/transformers/trainer_seq2seq.py:296\u001B[0m, in \u001B[0;36mSeq2SeqTrainer.prediction_step\u001B[0;34m(self, model, inputs, prediction_loss_only, ignore_keys, **gen_kwargs)\u001B[0m\n\u001B[1;32m    288\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[1;32m    289\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlabels\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m generation_inputs\n\u001B[1;32m    290\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdecoder_input_ids\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m generation_inputs\n\u001B[1;32m    291\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m generation_inputs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlabels\u001B[39m\u001B[38;5;124m\"\u001B[39m]\u001B[38;5;241m.\u001B[39mshape \u001B[38;5;241m==\u001B[39m generation_inputs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdecoder_input_ids\u001B[39m\u001B[38;5;124m\"\u001B[39m]\u001B[38;5;241m.\u001B[39mshape\n\u001B[1;32m    292\u001B[0m ):\n\u001B[1;32m    293\u001B[0m     generation_inputs \u001B[38;5;241m=\u001B[39m {\n\u001B[1;32m    294\u001B[0m         k: v \u001B[38;5;28;01mfor\u001B[39;00m k, v \u001B[38;5;129;01min\u001B[39;00m inputs\u001B[38;5;241m.\u001B[39mitems() \u001B[38;5;28;01mif\u001B[39;00m k \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m (\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdecoder_input_ids\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdecoder_attention_mask\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    295\u001B[0m     }\n\u001B[0;32m--> 296\u001B[0m generated_tokens \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgenerate\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mgeneration_inputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mgen_kwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    298\u001B[0m \u001B[38;5;66;03m# Temporary hack to ensure the generation config is not initialized for each iteration of the evaluation loop\u001B[39;00m\n\u001B[1;32m    299\u001B[0m \u001B[38;5;66;03m# TODO: remove this hack when the legacy code that initializes generation_config from a model config is\u001B[39;00m\n\u001B[1;32m    300\u001B[0m \u001B[38;5;66;03m# removed in https://github.com/huggingface/transformers/blob/98d88b23f54e5a23e741833f1e973fdf600cc2c5/src/transformers/generation/utils.py#L1183\u001B[39;00m\n\u001B[1;32m    301\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel\u001B[38;5;241m.\u001B[39mgeneration_config\u001B[38;5;241m.\u001B[39m_from_model_config:\n",
      "File \u001B[0;32m~/Documents/courses/NLP/Science-Quiz-Generation/venv/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001B[0m, in \u001B[0;36mcontext_decorator.<locals>.decorate_context\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    112\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(func)\n\u001B[1;32m    113\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdecorate_context\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m    114\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m ctx_factory():\n\u001B[0;32m--> 115\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/courses/NLP/Science-Quiz-Generation/venv/lib/python3.10/site-packages/transformers/generation/utils.py:1413\u001B[0m, in \u001B[0;36mGenerationMixin.generate\u001B[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001B[0m\n\u001B[1;32m   1405\u001B[0m         logger\u001B[38;5;241m.\u001B[39mwarning(\n\u001B[1;32m   1406\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mA decoder-only architecture is being used, but right-padding was detected! For correct \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1407\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgeneration results, please set `padding_side=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mleft\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m` when initializing the tokenizer.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1408\u001B[0m         )\n\u001B[1;32m   1410\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39mis_encoder_decoder \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mencoder_outputs\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m model_kwargs:\n\u001B[1;32m   1411\u001B[0m     \u001B[38;5;66;03m# if model is encoder decoder encoder_outputs are created\u001B[39;00m\n\u001B[1;32m   1412\u001B[0m     \u001B[38;5;66;03m# and added to `model_kwargs`\u001B[39;00m\n\u001B[0;32m-> 1413\u001B[0m     model_kwargs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_prepare_encoder_decoder_kwargs_for_generation\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1414\u001B[0m \u001B[43m        \u001B[49m\u001B[43minputs_tensor\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel_kwargs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel_input_name\u001B[49m\n\u001B[1;32m   1415\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1417\u001B[0m \u001B[38;5;66;03m# 5. Prepare `input_ids` which will be used for auto-regressive generation\u001B[39;00m\n\u001B[1;32m   1418\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39mis_encoder_decoder:\n",
      "File \u001B[0;32m~/Documents/courses/NLP/Science-Quiz-Generation/venv/lib/python3.10/site-packages/transformers/generation/utils.py:518\u001B[0m, in \u001B[0;36mGenerationMixin._prepare_encoder_decoder_kwargs_for_generation\u001B[0;34m(self, inputs_tensor, model_kwargs, model_input_name)\u001B[0m\n\u001B[1;32m    516\u001B[0m encoder_kwargs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mreturn_dict\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m    517\u001B[0m encoder_kwargs[model_input_name] \u001B[38;5;241m=\u001B[39m inputs_tensor\n\u001B[0;32m--> 518\u001B[0m model_kwargs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mencoder_outputs\u001B[39m\u001B[38;5;124m\"\u001B[39m]: ModelOutput \u001B[38;5;241m=\u001B[39m \u001B[43mencoder\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mencoder_kwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    520\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m model_kwargs\n",
      "File \u001B[0;32m~/Documents/courses/NLP/Science-Quiz-Generation/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1509\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1510\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/courses/NLP/Science-Quiz-Generation/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1515\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1516\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1518\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1519\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1520\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1523\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/Documents/courses/NLP/Science-Quiz-Generation/venv/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py:1207\u001B[0m, in \u001B[0;36mBartEncoder.forward\u001B[0;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[1;32m   1199\u001B[0m         layer_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_gradient_checkpointing_func(\n\u001B[1;32m   1200\u001B[0m             encoder_layer\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__call__\u001B[39m,\n\u001B[1;32m   1201\u001B[0m             hidden_states,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1204\u001B[0m             output_attentions,\n\u001B[1;32m   1205\u001B[0m         )\n\u001B[1;32m   1206\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1207\u001B[0m         layer_outputs \u001B[38;5;241m=\u001B[39m \u001B[43mencoder_layer\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1208\u001B[0m \u001B[43m            \u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1209\u001B[0m \u001B[43m            \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1210\u001B[0m \u001B[43m            \u001B[49m\u001B[43mlayer_head_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[43m[\u001B[49m\u001B[43midx\u001B[49m\u001B[43m]\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mis\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mnot\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1211\u001B[0m \u001B[43m            \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1212\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1214\u001B[0m     hidden_states \u001B[38;5;241m=\u001B[39m layer_outputs[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m   1216\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m output_attentions:\n",
      "File \u001B[0;32m~/Documents/courses/NLP/Science-Quiz-Generation/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1509\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1510\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/courses/NLP/Science-Quiz-Generation/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1515\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1516\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1518\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1519\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1520\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1523\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/Documents/courses/NLP/Science-Quiz-Generation/venv/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py:675\u001B[0m, in \u001B[0;36mBartEncoderLayer.forward\u001B[0;34m(self, hidden_states, attention_mask, layer_head_mask, output_attentions)\u001B[0m\n\u001B[1;32m    673\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mactivation_fn(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfc1(hidden_states))\n\u001B[1;32m    674\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m nn\u001B[38;5;241m.\u001B[39mfunctional\u001B[38;5;241m.\u001B[39mdropout(hidden_states, p\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mactivation_dropout, training\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtraining)\n\u001B[0;32m--> 675\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfc2\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    676\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m nn\u001B[38;5;241m.\u001B[39mfunctional\u001B[38;5;241m.\u001B[39mdropout(hidden_states, p\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdropout, training\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtraining)\n\u001B[1;32m    677\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m residual \u001B[38;5;241m+\u001B[39m hidden_states\n",
      "File \u001B[0;32m~/Documents/courses/NLP/Science-Quiz-Generation/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1509\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1510\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/courses/NLP/Science-Quiz-Generation/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1515\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1516\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1518\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1519\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1520\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1523\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/Documents/courses/NLP/Science-Quiz-Generation/venv/lib/python3.10/site-packages/torch/nn/modules/linear.py:116\u001B[0m, in \u001B[0;36mLinear.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    115\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[0;32m--> 116\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlinear\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "def compute_bleu_score(targets, predictions):\n",
    "    total_bleu_score = 0\n",
    "    for target_sentence, prediction in zip(targets, predictions):\n",
    "        predicted_sentence = tokenizer.decode(prediction, skip_special_tokens=True)\n",
    "        bleu_score = sentence_bleu(target_sentence, predicted_sentence)\n",
    "        print(bleu_score, target_sentence, predicted_sentence)\n",
    "        total_bleu_score += bleu_score\n",
    "    total_bleu_score /= len(targets)\n",
    "    print(total_bleu_score)\n",
    "    \n",
    "predictions = trainer.predict(test_data)\n",
    "compute_bleu_score(test_data, predictions[0])\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-19T14:59:09.340711Z",
     "start_time": "2024-03-19T14:59:05.659010Z"
    }
   },
   "execution_count": 68
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2000064705012679e-231 A habitat’s features are determined mainly by abiotic factors such as? What is the physical environment in which a species lives and to which it is adapted?\n",
      "1.2217404365441333e-231 What part of the body does caffeine stimulate? Some psychoactive drugs, such as caffeine, stimulate the central nervous system. they may\n",
      "1.3067701985851573e-231 An extensive property is a property that depends on the amount of what in a sample? What is a property that depends on the amount of matter in a sample?\n",
      "1.2882297539194154e-231 Electron capture occurs when an inner shell electron combines with a proton and is converted into what? What does air passing between the vehicles flow in a narrower channel cause?\n",
      "1.362137122503591e-231 What is an area in a body of water where nothing grows because there is too little oxygen known as? What does rain dissolves fertilizer in the soil?\n",
      "1.2383665771889249e-231 Usually done on computers, what do you call sets of equations that take into account many factors to represent a phenomenon? What are sets of equations that take into account many factors to represent a phenomenon?\n",
      "1.1926689993037216e-231 Mutations in what type of regulatory gene can cause misplacement of structures in an animal? When organisms penetrate a rock, they accelerate breakdown by chemical means and what other means?\n",
      "1.2183324802375697e-231 Even the age of earth has been estimated on the basis of what? What is used to estimate the ages of not only of rocks, but also of fossils\n",
      "1.355497751227727e-231 A closed loop through which a current can flow is called what? What is the closed loop through which current can flow called?\n",
      "1.3135841289152546e-231 What type of growth generally occurs only when a population is living under ideal conditions, yet cannot continue for very long? What happens to growth when a population is living under ideal conditions?\n",
      "1.2960139736913823e-231 Manganin is made up of metals that include copper, manganese and nickel - what do you generally call a metal of this type? Some alloys have a small temperature dependence; for example, manganese and nickel\n",
      "1.4619233061303572e-231 The radioactive gas radon and uv radiation are culprits in different types of what disease? What is the leading cause of skin cancer?\n",
      "1.1926689993037216e-231 Most of the food that they bring to their chicks is very rich in what? When organisms penetrate a rock, they accelerate breakdown by chemical means and what other means?\n",
      "1.3165594234639305e-231 What is the number waves that pass a fixed point in a given amount of time called? What is the number of waves that pass a fixed point in a given amount of time\n",
      "1.3404899290088416e-231 Destructive interference occurs when two identical waves are superimposed exactly out of phase. a standing wave is one in which two waves superimpose to produce a wave that varies in amplitude but does not do this? What is the combination of two waves at the same location?\n",
      "1.232584406082509e-231 An individual grows quickly and develops new abilities during infancy and? When an individual grows quickly and develops new abilities during infancy and childhood, what happens to\n",
      "1.4637115948630222e-231 What is another term for seed plants? What type of plants are seed plants?\n",
      "1.2454230825798051e-231 What kind of mixture consists of two or more phases, exemplified when a combination of oil and water forms layers? What is the term for any part of a sample that has a uniform composition and properties\n",
      "1.3545058019712e-231 What does the aqueous fluid between the chloroplast membrane and the grana known as? Between the chloroplast membrane and the grana is an aqueous fluid known\n",
      "1.224501449487111e-231 What connections allow heterocysts to transport fixed nitrogen to neighboring cells and to receive carbohydrates? When organisms penetrate a rock, they accelerate breakdown by chemical means and what other means?\n",
      "1.1673030940407554e-231 Increasing the surface area of solid reactants increases what rate? When a solid substance is involved in a chemical reaction, only the matter at the surface\n",
      "1.404215963202711e-231 Cardiac muscle normally has what kind of oxygen-using metabolism? What type of muscle metabolism is entirely aerobic?\n",
      "1.3103343818443682e-231 What do you call materials that have low resistance to electric current? What are materials that have low resistance to electric current called?\n",
      "1.1965027645843237e-231 If more coils are added to an electromagnet it becomes? What is the combined magnetic force of the magnetized wire coil and iron bar making an\n",
      "1.2265928344754895e-231 Parasitic chelicerates like ticks and mites have evolved what? Most chelicerates ingest food using a preoral cavity formed by the chelic\n",
      "1.2256064301115746e-231 A frameshift mutation is a deletion or insertion of one or more of what that changes the reading frame of the base sequence? What is a deletion or insertion of one or more nucleotides that changes the reading\n",
      "1.312319241431207e-231 What type of power is generated by splitting uranium atoms? What type of power plants use uranium that has been concentrated in fuel rods?\n",
      "1.2395288183339461e-231 Compounds with aluminum and silicon are commonly found in the clay fractions of soils derived from what? Compounds with aluminum and silicon are commonly found in the clay fractions of soils derived from\n",
      "1.2116564345737439e-231 Wind or water that travels toward the poles from the equator curves in which direction? What effect bends the direction of surface currents to the right in the Northern Hemisphere?\n",
      "1.2183324802375697e-231 During what process is light from the star focused and the star appears to brighten in a characteristic manner? Light from the star is focused and the star appears to brighten in a characteristic manner\n",
      "1.2151472700333379e-231 An electrostatic attraction between two ions that have exchanged what? What is the term for an electrostatic attraction between two ions that have exchanged electrons?\n",
      "1.4132082004247135e-231 What type of molecules do hydrogen bonds hold together? What type of bonds hold adjacent water molecules together?\n",
      "1.1926689993037216e-231 Instead of heat, organisms use what to speed up reactions? When organisms penetrate a rock, they accelerate breakdown by chemical means and what other means?\n",
      "1.4068091708552948e-231 What are clathrin, copi and copii types of? What coat selects specific proteins as cargo?\n",
      "1.3040392114580104e-231 What do you call winds that occur in belts that go all around the planet? Which type of winds are winds that occur in belts that go all around the planet?\n",
      "1.459387436556547e-231 What type of fossils are of organisms that lived over a wide area for a fairly short period of time and are used to determine the age of the rock it is in? What are used to determine the ages of rock layers?\n",
      "1.2051593831051636e-231 What are hydrocarbons most important use? What type of chemical substance has a wide variety of important uses, but their most important\n",
      "1.334036725854817e-231 The burning of fossil fuels releases three major things - thermal energy, water vapor, and what pollutant? When fossil fuels burn, they release thermal energy, water vapor, and what else?\n",
      "1.384292958842266e-231 An environment reaches its carrying capacity when the number of individual births in it equal the number of what else? Where is the carrying capacity of an environment reached?\n",
      "1.384292958842266e-231 Most air pollutants can be traced to what source ? Most air pollutants can be traced to the burning of what?\n",
      "1.4531425888420502e-231 Total internal reflections the princicple behind what type of optics? What is the principle behind fiber optics?\n",
      "1.1926689993037216e-231 Salmon must do what when they migrate from freshwater to the ocean? When organisms penetrate a rock, they accelerate breakdown by chemical means and what other means?\n",
      "1.384292958842266e-231 What is the maximum horizontal distance traveled by a projectile? What is the maximum horizontal distance traveled by a projectile called?\n",
      "1.407181588934329e-231 What is defined as the change in the size of the population over time? What is the change in the size of the population over time?\n",
      "1.3028433924917136e-231 In genetics, what does it mean when the amount is longer since the amount of time since a species diverged? What is used to compare the dna or proteins of different species?\n",
      "1.4097034298540582e-231 The preferred phase a substance adopts can change with temperature. at low temperatures, most substances are solids (only helium is predicted to be a liquid at absolute zero). as the temperature increases, those substances with very weak intermolecular forces become gases directly in a process called this? What is the preferred phase for any random substance?\n",
      "1.3483065280626046e-231 What do plant viruses have that protect their cells? What do many plant viruses lack a core of either dna or RNA?\n",
      "1.4111849909420337e-231 What term is used to describe elements with unstable nuclei? What are produced when radioactive elements decay?\n",
      "1.479368491365877e-231 What are the long carbon chains that make up lipids? What are made up of long carbon chains called?\n",
      "1.325250056439003e-231 What type of tissue makes up the brain and the nerves that connect the brain to all parts of the body? What is made up of neurons, or nerve cells, that carry electrical messages?\n",
      "1.2973344175574298e-231 Branchiopoda are mostly small, freshwater animals that feed on? What are small, freshwater animals that feed on plankton and detritus?\n",
      "1.3435336867013266e-231 The solubility of gases, liquids, and solids are affected by changes in what? The solubility of gases, liquids, and solids are affected by changes in\n",
      "1.362137122503591e-231 The pubis forms the anterior portion of what bone? What forms the anterior portion of the hip bone?\n",
      "1.308146472423365e-231 What is it called when two organisms live close together and form a relationship? What is it called when two species live close together and form a relationship?\n",
      "1.2155791689002139e-231 The distribution of thermal speeds depends strongly on temperature. as temperature increases, the speeds are shifted to higher values and the distribution is what? Temperature increases as temperature increases, the speeds are shifted to higher values and the distribution is\n",
      "1.37758916398409e-231 Skin that acts as camouflage or secretes chemicals poisonous to predators are successful modification in frogs and salamanders, classed as what in the order anura? What are amphibians that belong to the order annura?\n",
      "1.2973344175574298e-231 Compound machines tend to have a greater mechanical advantage than what other machines? What is the factor by which it changes the force applied to a machine?\n",
      "1.379716721154912e-231 What mineral is used in jewelry because of its striking greenish-blue color? What mineral is used in jewelry because of its striking greenish-blue color?\n",
      "1.2882297539194154e-231 Which construction material previously used in factories and in homes caused cancer? What is the term for the past use of asbestos in factories and in homes?\n",
      "1.2363867681772576e-231 Spicules are most conspicuously present in which class? Class Calcarea contains calcium carbonate spicules and no spongin,\n",
      "1.2563970334230793e-231 Temperature is a measure of how hot or cold an object is relative to another object, whereas heat is the flow of what energy between objects with different temperatures? What term is used to measure how hot or cold an object is relative to another object\n",
      "1.445061779299676e-231 Cnidarians are an ancient phylum of what? Cnidarians are an ancient phylum of eumetazoans.\n",
      "1.3067701985851573e-231 The numbers and types of species living in what groups generally change through time, a process called ecological succession? What are the two main types of succession that occur in communities?\n",
      "1.158061971064715e-231 What can have complex effects on soil nutrient concentrations? When organisms penetrate a rock, they accelerate breakdown by chemical means and what other means?\n",
      "1.1837121527700106e-231 What are groups of skeletal muscle fibers wrapped in? Each skeletal muscle consists of hundreds or even thousands of skeletal muscle fibers. the fibers are\n",
      "1.2265928344754895e-231 What kind of feeders are sponges? What are the specialized collar cells that are also known as choanocytes?\n",
      "1.3388441433044314e-231 Protists play critically important ecological roles as producers and, on the other end of food webs, as what? What are photosynthetic dinoflagellates that house them responsible for?\n",
      "1.5656618337072542e-231 What is defined as the ability to cause changes in matter? What human ear is pictured below?\n",
      "1.3344759149598398e-231 What is the fusing of two or more smaller nuclei to form a single, larger nucleus? What is the fusing of two or more smaller nuclei to form a single,\n",
      "1.2882297539194154e-231 Fragmentation with subsequent regeneration is a method of what, exhibited by animals such as sea stars? What is the breaking of the body into two parts with subsequent regeneration called?\n",
      "1.2601643831470213e-231 Because the current is alternating, the magnetic field of the iron core keeps doing what? What is the transformer that has two wire coils wrapped around an iron core called?\n",
      "1.2088995192503553e-231 Steroid hormones, such as cortisol and ecdysteroid, are lipids that contain four fused what? When organisms penetrate a rock, they accelerate breakdown by chemical means and what other means?\n",
      "1.3435336867013266e-231 What is the term for the distance that sound waves travel in a given amount of time? What is the distance that sound waves travel in a given amount of time?\n",
      "1.3267266604633973e-231 What is the term for a homogeneous mixture of two or more substances? What is formed when rocks or other substances dissolve in water?\n",
      "1.4875195904069663e-231 What is an organic compound that is the primary component of natural gas? What is the primary component of natural gas?\n",
      "1.17792365583896e-231 Kinetic theory is the atomistic description of what as well as liquids and solids? What theory models the properties of matter in terms of continuous random motion of atoms and molecules\n",
      "1.2035620841904511e-231 An equipotential line is a line along which the electric potential is wht? What is the process by which a conductor can be fixed at zero volts by connecting it\n",
      "1.583976781977924e-231 In budding , organisms reproduce by having new individuals split off from what? In what way do organisms reproduce?\n",
      "1.1757474215006176e-231 Instead of insects, crustaceans are the dominant arthropods in what environment? When organisms penetrate a rock, they accelerate breakdown by chemical means and what other means?\n",
      "1.2847527188296715e-231 What occurs when light bumps into tiny particles of matter and spreads out in all directions? What occurs when light bumps into tiny particles of matter and spreads out in all directions?\n",
      "1.224501449487111e-231 Body movements help circulate the hemolymph by periodically squeezing what? When organisms penetrate a rock, they accelerate breakdown by chemical means and what other means?\n",
      "1.2936539991537172e-231 What two things are excess proteins converted into? What is the production of proteins from amino acids called?\n",
      "1.1926689993037216e-231 By maintaining a relatively constant internal environment even when the external environment changes significantly, an animal achieves what? When organisms penetrate a rock, they accelerate breakdown by chemical means and what other means?\n",
      "1.3180597785174428e-231 The binary halides are an important subclass of what? The binary halides are an important subclass of salts. a salt is an ionic\n",
      "1.3483065280626046e-231 What is touching a charged object to the earth called? What is the term for touching a charged object to the earth?\n",
      "1.4256605770826504e-231 What are animals that eat a prey animal? What are animals that eat a prey animal?\n",
      "1.4829635810350492e-231 By what processes do rivers create floodplains? What is deposited by slow-flowing rivers?\n",
      "1.2183324802375697e-231 What are collisions between gas particles and container walls called? What are collisions between gas particles and between particles and the container walls called?\n",
      "1.3435336867013266e-231 What is the process called in which populations of organisms change over time? What is the process by which populations of organisms change over time?\n",
      "1.350421525194928e-231 The post-anal tail is at the end of the organism opposite what? What is at the end of the organism opposite the head?\n",
      "1.4666302765429156e-231 What phylum includes sponges, which are aquatic invertebrates? What are aquatic invertebrates in phylum Porifera?\n",
      "1.337299023427136e-231 Burning fossil fuels releases what into the atmosphere? Burning organic material, such as fossil fuels, releases what?\n",
      "1.1640469867513693e-231 Within a particular habitat, what can be characterized by its size or density? Population size and densityThe study of any population usually begins by determining how many individuals of\n",
      "1.404215963202711e-231 The outer ear, or ear canal, carries sound to the recessed protected what? The outer ear, or ear canal, carries sound to what?\n",
      "1.3782872078685485e-231 What are the organisms that live in extreme conditions known as? What are organisms that live in extreme conditions called?\n",
      "1.3592502031561055e-231 What is the three-dimensional structure of a single polypeptide called? What refers to the three-dimensional structure of a single polypeptide?\n",
      "1.1926689993037216e-231 The energy stored in the organic molecules of food ultimately comes from where? When organisms penetrate a rock, they accelerate breakdown by chemical means and what other means?\n",
      "1.3215941593894153e-231 Momentum can be expressed as the product of mass and what else? What is the product of the mass and velocity of an object called?\n",
      "1.224501449487111e-231 Which blood pressure is highest when the heart contracts during ventricular systole? When organisms penetrate a rock, they accelerate breakdown by chemical means and what other means?\n",
      "1.1977335250982776e-231 What makes nobel gases unreactive? What rule states that elements tend to form compounds in ways that give each atom eight val\n",
      "1.3085802587839636e-231\n"
     ]
    }
   ],
   "source": [
    "compute_bleu_score(test_data['question'], predictions[0])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-19T14:59:12.592786Z",
     "start_time": "2024-03-19T14:59:12.258335Z"
    }
   },
   "execution_count": 69
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
